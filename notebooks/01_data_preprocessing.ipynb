{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "090fdfea",
   "metadata": {},
   "source": [
    "Set up the Notebook, Load Data, and Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaa6d87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded coin_gecko_2022-03-16.csv\n",
      "Successfully loaded coin_gecko_2022-03-17.csv\n",
      "\n",
      "Combined DataFrame created.\n",
      "Total rows in combined DataFrame: 1000\n",
      "\n",
      "--- First 5 rows of the combined data ---\n",
      "       coin symbol         price     1h    24h     7d    24h_volume  \\\n",
      "0   Bitcoin    BTC  40859.460000  0.022  0.030  0.055  3.539076e+10   \n",
      "1  Ethereum    ETH   2744.410000  0.024  0.034  0.065  1.974870e+10   \n",
      "2    Tether   USDT      1.000000 -0.001 -0.001  0.000  5.793497e+10   \n",
      "3       BNB    BNB    383.430000  0.018  0.028  0.004  1.395854e+09   \n",
      "4  USD Coin   USDC      0.999874 -0.001  0.000 -0.000  3.872274e+09   \n",
      "\n",
      "        mkt_cap        date  \n",
      "0  7.709915e+11  2022-03-16  \n",
      "1  3.271044e+11  2022-03-16  \n",
      "2  7.996516e+10  2022-03-16  \n",
      "3  6.404382e+10  2022-03-16  \n",
      "4  5.222214e+10  2022-03-16  \n",
      "\n",
      "--- Data Info (Data Types and Non-Null Counts) ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 9 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   coin        1000 non-null   object \n",
      " 1   symbol      1000 non-null   object \n",
      " 2   price       1000 non-null   float64\n",
      " 3   1h          993 non-null    float64\n",
      " 4   24h         993 non-null    float64\n",
      " 5   7d          992 non-null    float64\n",
      " 6   24h_volume  993 non-null    float64\n",
      " 7   mkt_cap     1000 non-null   float64\n",
      " 8   date        1000 non-null   object \n",
      "dtypes: float64(6), object(3)\n",
      "memory usage: 70.4+ KB\n",
      "None\n",
      "\n",
      "--- Basic Statistical Summary ---\n",
      "              price          1h         24h          7d    24h_volume  \\\n",
      "count  1.000000e+03  993.000000  993.000000  992.000000  9.930000e+02   \n",
      "mean   6.561060e+02    0.009723    0.023737    0.023558  2.893109e+08   \n",
      "std    4.584655e+03    0.026934    0.059303    0.229781  2.769908e+09   \n",
      "min    1.484000e-09   -0.704000   -0.646000   -0.558000  0.000000e+00   \n",
      "25%    1.940547e-01    0.001000    0.001000   -0.041000  1.766976e+06   \n",
      "50%    1.095000e+00    0.006000    0.016000   -0.000500  8.343005e+06   \n",
      "75%    7.232500e+00    0.019000    0.035000    0.037000  4.003652e+07   \n",
      "max    4.121727e+04    0.095000    0.577000    4.608000  5.793497e+10   \n",
      "\n",
      "            mkt_cap  \n",
      "count  1.000000e+03  \n",
      "mean   3.755304e+09  \n",
      "std    3.803783e+10  \n",
      "min    6.577043e+07  \n",
      "25%    1.157776e+08  \n",
      "50%    2.120036e+08  \n",
      "75%    5.948436e+08  \n",
      "max    7.760774e+11  \n",
      "\n",
      "--- Missing Values Count per Column ---\n",
      "coin          0\n",
      "symbol        0\n",
      "price         0\n",
      "1h            7\n",
      "24h           7\n",
      "7d            8\n",
      "24h_volume    7\n",
      "mkt_cap       0\n",
      "date          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np # Import numpy for NaN handling\n",
    "\n",
    "# Define the path to your raw data directory\n",
    "# Adjust this path if your notebook is not in 'notebooks/' relative to 'data/'\n",
    "raw_data_path = '../data/raw/'\n",
    "\n",
    "# List the CSV files you want to load\n",
    "file_names = ['coin_gecko_2022-03-16.csv', 'coin_gecko_2022-03-17.csv']\n",
    "\n",
    "# Create an empty list to hold dataframes\n",
    "dfs = []\n",
    "\n",
    "# Load each CSV file\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(raw_data_path, file_name)\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        dfs.append(df)\n",
    "        print(f\"Successfully loaded {file_name}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {file_name} not found at {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_name}: {e}\")\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "if dfs:\n",
    "    raw_df = pd.concat(dfs, ignore_index=True)\n",
    "    print(\"\\nCombined DataFrame created.\")\n",
    "    print(f\"Total rows in combined DataFrame: {raw_df.shape[0]}\")\n",
    "else:\n",
    "    raw_df = pd.DataFrame()\n",
    "    print(\"No dataframes were loaded to combine.\")\n",
    "\n",
    "# --- Initial Data Inspection ---\n",
    "\n",
    "print(\"\\n--- First 5 rows of the combined data ---\")\n",
    "print(raw_df.head())\n",
    "\n",
    "print(\"\\n--- Data Info (Data Types and Non-Null Counts) ---\")\n",
    "print(raw_df.info())\n",
    "\n",
    "print(\"\\n--- Basic Statistical Summary ---\")\n",
    "print(raw_df.describe())\n",
    "\n",
    "print(\"\\n--- Missing Values Count per Column ---\")\n",
    "print(raw_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570a3c6d",
   "metadata": {},
   "source": [
    "Date Column Processing and Initial Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e3b5683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'date' column converted to datetime and set as index.\n",
      "'price' column converted to numeric.\n",
      "Removed 2 duplicate rows.\n",
      "\n",
      "--- DataFrame Info after Date Processing and Duplicates Handling ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 998 entries, 2022-03-16 to 2022-03-17\n",
      "Data columns (total 8 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   coin        998 non-null    object \n",
      " 1   symbol      998 non-null    object \n",
      " 2   price       998 non-null    float64\n",
      " 3   1h          993 non-null    float64\n",
      " 4   24h         993 non-null    float64\n",
      " 5   7d          992 non-null    float64\n",
      " 6   24h_volume  993 non-null    float64\n",
      " 7   mkt_cap     998 non-null    float64\n",
      "dtypes: float64(6), object(2)\n",
      "memory usage: 70.2+ KB\n",
      "None\n",
      "\n",
      "--- Missing Values after Initial Cleaning ---\n",
      "coin          0\n",
      "symbol        0\n",
      "price         0\n",
      "1h            5\n",
      "24h           5\n",
      "7d            6\n",
      "24h_volume    5\n",
      "mkt_cap       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- IMPORTANT: Replace 'Date_Column_Name' and 'Price_Column_Name' with actual column names from your data ---\n",
    "\n",
    "# Make a copy to work on, preserving the raw_df\n",
    "df = raw_df.copy()\n",
    "\n",
    "# 2.1 Convert 'Date' column to datetime and set as index\n",
    "date_column_name = 'date' # <--- REPLACE WITH YOUR ACTUAL DATE COLUMN NAME (e.g., 'Date', 'timestamp')\n",
    "\n",
    "if date_column_name in df.columns:\n",
    "    df[date_column_name] = pd.to_datetime(df[date_column_name], errors='coerce')\n",
    "    # Drop rows where date conversion failed (if any, these will turn into NaT - Not a Time)\n",
    "    df.dropna(subset=[date_column_name], inplace=True)\n",
    "    # Sort by date to ensure chronological order for time series analysis\n",
    "    df.sort_values(by=date_column_name, inplace=True)\n",
    "    # Set the date column as the DataFrame index\n",
    "    df.set_index(date_column_name, inplace=True)\n",
    "    print(f\"\\n'{date_column_name}' column converted to datetime and set as index.\")\n",
    "else:\n",
    "    print(f\"Warning: '{date_column_name}' not found. Please verify the date column name from Step 1 output.\")\n",
    "\n",
    "# 2.2 Identify Price Column and ensure it's numeric\n",
    "price_column_name = 'price' # <--- REPLACE WITH YOUR ACTUAL PRICE COLUMN NAME (e.g., 'Close', 'priceUsd')\n",
    "\n",
    "if price_column_name in df.columns:\n",
    "    # Coerce errors will turn non-numeric values into NaN, which we handle next\n",
    "    df[price_column_name] = pd.to_numeric(df[price_column_name], errors='coerce')\n",
    "    print(f\"'{price_column_name}' column converted to numeric.\")\n",
    "else:\n",
    "    print(f\"Error: '{price_column_name}' column not found. Please verify the price column name from Step 1 output. This is our target column!\")\n",
    "\n",
    "# 2.3 Handle Duplicate Rows (if any)\n",
    "initial_rows = df.shape[0]\n",
    "df.drop_duplicates(inplace=True)\n",
    "if df.shape[0] < initial_rows:\n",
    "    print(f\"Removed {initial_rows - df.shape[0]} duplicate rows.\")\n",
    "else:\n",
    "    print(\"No duplicate rows found.\")\n",
    "\n",
    "# Display info after initial processing\n",
    "print(\"\\n--- DataFrame Info after Date Processing and Duplicates Handling ---\")\n",
    "print(df.info())\n",
    "print(\"\\n--- Missing Values after Initial Cleaning ---\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e4d476",
   "metadata": {},
   "source": [
    "Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a53ab728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numerical columns identified for imputation: ['price', '1h', '24h', '7d', '24h_volume', 'mkt_cap']\n",
      "\n",
      "Filling missing values using forward fill (ffill)...\n",
      "Filling any remaining missing values using backward fill (bfill)...\n",
      "No missing values found in 'price' after imputation.\n",
      "\n",
      "--- Missing Values Count after Imputation ---\n",
      "coin          0\n",
      "symbol        0\n",
      "price         0\n",
      "1h            0\n",
      "24h           0\n",
      "7d            0\n",
      "24h_volume    0\n",
      "mkt_cap       0\n",
      "dtype: int64\n",
      "DataFrame shape after imputation and dropping price NaNs: (998, 8)\n"
     ]
    }
   ],
   "source": [
    "# Identify numerical columns for imputation (excluding the datetime index)\n",
    "numerical_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumerical columns identified for imputation: {numerical_cols}\")\n",
    "\n",
    "# Strategy 1: Forward-fill (ffill) for time-series data\n",
    "print(\"\\nFilling missing values using forward fill (ffill)...\")\n",
    "df[numerical_cols] = df[numerical_cols].ffill()\n",
    "\n",
    "# Strategy 2: Backward-fill (bfill) for any remaining NaNs (e.g., at the very beginning of the series)\n",
    "print(\"Filling any remaining missing values using backward fill (bfill)...\")\n",
    "df[numerical_cols] = df[numerical_cols].bfill()\n",
    "\n",
    "# Special handling for the target column ('price_column_name') if it still has NaNs\n",
    "# This can happen if the entire series or a block at the very start/end was missing for price\n",
    "if df[price_column_name].isnull().any():\n",
    "    print(f\"\\nWarning: Some missing values remain in '{price_column_name}' even after ffill and bfill.\")\n",
    "    # Option: Drop rows where the target price is still NaN (recommended for target variable)\n",
    "    missing_price_rows_before_drop = df[price_column_name].isnull().sum()\n",
    "    df.dropna(subset=[price_column_name], inplace=True)\n",
    "    print(f\"Dropped {missing_price_rows_before_drop} rows with remaining missing '{price_column_name}' values.\")\n",
    "else:\n",
    "    print(f\"No missing values found in '{price_column_name}' after imputation.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Missing Values Count after Imputation ---\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"DataFrame shape after imputation and dropping price NaNs: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54f40dd",
   "metadata": {},
   "source": [
    " Basic Feature Engineering (Time-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21a7590c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating time-based features...\n",
      "Time-based features created: year, month, day_of_week, day_of_year, week_of_year, quarter.\n",
      "\n",
      "--- First 5 rows with new time-based features ---\n",
      "                      coin  symbol         price     1h    24h     7d  \\\n",
      "date                                                                    \n",
      "2022-03-16         Bitcoin     BTC  4.085946e+04  0.022  0.030  0.055   \n",
      "2022-03-16  Iron Bank EURO   IBEUR  1.080000e+00  0.000 -0.004  0.009   \n",
      "2022-03-16       Prometeus    PROM  7.960000e+00  0.017  0.008  0.015   \n",
      "2022-03-16    MaidSafeCoin    MAID  2.949200e-01  0.023  0.010  0.045   \n",
      "2022-03-16    Bezoge Earth  BEZOGE  3.051000e-09  0.012 -0.005 -0.041   \n",
      "\n",
      "              24h_volume       mkt_cap  year  month  day_of_week  day_of_year  \\\n",
      "date                                                                            \n",
      "2022-03-16  3.539076e+10  7.709915e+11  2022      3            2           75   \n",
      "2022-03-16  9.525810e+04  1.300442e+08  2022      3            2           75   \n",
      "2022-03-16  1.069360e+06  1.302007e+08  2022      3            2           75   \n",
      "2022-03-16  3.041720e+03  1.327759e+08  2022      3            2           75   \n",
      "2022-03-16  1.894020e+05  1.329136e+08  2022      3            2           75   \n",
      "\n",
      "            week_of_year  quarter  \n",
      "date                               \n",
      "2022-03-16            11        1  \n",
      "2022-03-16            11        1  \n",
      "2022-03-16            11        1  \n",
      "2022-03-16            11        1  \n",
      "2022-03-16            11        1  \n"
     ]
    }
   ],
   "source": [
    "# Ensure the index is datetime for feature extraction\n",
    "if not pd.api.types.is_datetime64_any_dtype(df.index):\n",
    "    print(\"\\nError: DataFrame index is not datetime. Please ensure Step 2 was completed correctly.\")\n",
    "else:\n",
    "    print(\"\\nGenerating time-based features...\")\n",
    "    df['year'] = df.index.year\n",
    "    df['month'] = df.index.month\n",
    "    df['day_of_week'] = df.index.dayofweek # Monday=0, Sunday=6\n",
    "    df['day_of_year'] = df.index.dayofyear\n",
    "    # .isocalendar().week returns a Series, need to convert to int if not already\n",
    "    df['week_of_year'] = df.index.isocalendar().week.astype(int)\n",
    "    df['quarter'] = df.index.quarter\n",
    "    print(\"Time-based features created: year, month, day_of_week, day_of_year, week_of_year, quarter.\")\n",
    "\n",
    "print(\"\\n--- First 5 rows with new time-based features ---\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f709f027",
   "metadata": {},
   "source": [
    "Normalize/Scale Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "032f9ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features identified for scaling: ['1h', '24h', '7d', '24h_volume', 'mkt_cap']\n",
      "Numerical features scaled using MinMaxScaler.\n",
      "\n",
      "--- First 5 rows after scaling (showing scaled features) ---\n",
      "                      coin  symbol         price        1h       24h  \\\n",
      "date                                                                   \n",
      "2022-03-16         Bitcoin     BTC  4.085946e+04  0.908636  0.552739   \n",
      "2022-03-16  Iron Bank EURO   IBEUR  1.080000e+00  0.881101  0.524939   \n",
      "2022-03-16       Prometeus    PROM  7.960000e+00  0.902378  0.534751   \n",
      "2022-03-16    MaidSafeCoin    MAID  2.949200e-01  0.909887  0.536386   \n",
      "2022-03-16    Bezoge Earth  BEZOGE  3.051000e-09  0.896120  0.524121   \n",
      "\n",
      "                  7d    24h_volume   mkt_cap  year  month  day_of_week  \\\n",
      "date                                                                     \n",
      "2022-03-16  0.118660  6.108705e-01  0.993446  2022      3            2   \n",
      "2022-03-16  0.109756  1.644225e-06  0.000083  2022      3            2   \n",
      "2022-03-16  0.110918  1.845794e-05  0.000083  2022      3            2   \n",
      "2022-03-16  0.116725  5.250232e-08  0.000086  2022      3            2   \n",
      "2022-03-16  0.100077  3.269217e-06  0.000087  2022      3            2   \n",
      "\n",
      "            day_of_year  week_of_year  quarter  \n",
      "date                                            \n",
      "2022-03-16           75            11        1  \n",
      "2022-03-16           75            11        1  \n",
      "2022-03-16           75            11        1  \n",
      "2022-03-16           75            11        1  \n",
      "2022-03-16           75            11        1  \n",
      "\n",
      "--- Final DataFrame Info after all preprocessing steps ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 998 entries, 2022-03-16 to 2022-03-17\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   coin          998 non-null    object \n",
      " 1   symbol        998 non-null    object \n",
      " 2   price         998 non-null    float64\n",
      " 3   1h            998 non-null    float64\n",
      " 4   24h           998 non-null    float64\n",
      " 5   7d            998 non-null    float64\n",
      " 6   24h_volume    998 non-null    float64\n",
      " 7   mkt_cap       998 non-null    float64\n",
      " 8   year          998 non-null    int32  \n",
      " 9   month         998 non-null    int32  \n",
      " 10  day_of_week   998 non-null    int32  \n",
      " 11  day_of_year   998 non-null    int32  \n",
      " 12  week_of_year  998 non-null    int32  \n",
      " 13  quarter       998 non-null    int32  \n",
      "dtypes: float64(6), int32(6), object(2)\n",
      "memory usage: 93.6+ KB\n",
      "None\n",
      "\n",
      "--- Final Missing Values Count ---\n",
      "coin            0\n",
      "symbol          0\n",
      "price           0\n",
      "1h              0\n",
      "24h             0\n",
      "7d              0\n",
      "24h_volume      0\n",
      "mkt_cap         0\n",
      "year            0\n",
      "month           0\n",
      "day_of_week     0\n",
      "day_of_year     0\n",
      "week_of_year    0\n",
      "quarter         0\n",
      "dtype: int64\n",
      "Final DataFrame shape: (998, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Identify features to scale (all numerical columns except the target and time-based features)\n",
    "# Ensure price_column_name is defined from earlier steps\n",
    "features_to_scale = [col for col in df.select_dtypes(include=['number']).columns if col != price_column_name]\n",
    "\n",
    "# Exclude the time-based features from scaling as they are more like categorical/ordinal features\n",
    "# and often don't benefit from min-max scaling or are handled differently (e.g., one-hot encoding if truly categorical)\n",
    "time_based_features = ['year', 'month', 'day_of_week', 'day_of_year', 'week_of_year', 'quarter']\n",
    "features_to_scale = [f for f in features_to_scale if f not in time_based_features]\n",
    "\n",
    "print(f\"\\nFeatures identified for scaling: {features_to_scale}\")\n",
    "\n",
    "if features_to_scale:\n",
    "    # Initialize MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Fit and transform the selected features\n",
    "    # IMPORTANT NOTE for future steps:\n",
    "    # In a real machine learning pipeline, you should fit this scaler ONLY on your TRAINING data\n",
    "    # to prevent \"data leakage.\" Then, you apply the *fitted* scaler to both your training and test data.\n",
    "    # For now, we are applying it directly to the full 'df' for preprocessing demonstration.\n",
    "    df[features_to_scale] = scaler.fit_transform(df[features_to_scale])\n",
    "    print(\"Numerical features scaled using MinMaxScaler.\")\n",
    "else:\n",
    "    print(\"No numerical features identified for scaling (excluding target and time-based features).\")\n",
    "\n",
    "print(\"\\n--- First 5 rows after scaling (showing scaled features) ---\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n--- Final DataFrame Info after all preprocessing steps ---\")\n",
    "print(df.info())\n",
    "print(\"\\n--- Final Missing Values Count ---\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"Final DataFrame shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12dcd1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessed data successfully saved to: ../data/processed/preprocessed_crypto_data.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the path to the processed data directory\n",
    "processed_data_path = '../data/processed/'\n",
    "output_file_name = 'preprocessed_crypto_data.csv'\n",
    "output_file_path = os.path.join(processed_data_path, output_file_name)\n",
    "\n",
    "# Create the processed data directory if it doesn't exist\n",
    "os.makedirs(processed_data_path, exist_ok=True)\n",
    "\n",
    "# Save the preprocessed DataFrame to a CSV file\n",
    "# We'll save the index as well since it contains our datetime values\n",
    "try:\n",
    "    df.to_csv(output_file_path, index=True)\n",
    "    print(f\"\\nPreprocessed data successfully saved to: {output_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving preprocessed data: {e}\")\n",
    "\n",
    "# You can optionally verify by trying to load it back\n",
    "# verify_df = pd.read_csv(output_file_path, index_col=0, parse_dates=True)\n",
    "# print(\"\\nVerification: First 5 rows of the saved data:\")\n",
    "# print(verify_df.head())\n",
    "# print(\"\\nVerification: Info of the saved data:\")\n",
    "# print(verify_df.info())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
