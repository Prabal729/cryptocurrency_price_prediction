# ğŸ“Š Cryptocurrency Liquidity Prediction Project

This project aims to predict the **liquidity** (measured via price or volume) of various cryptocurrencies using historical data from CoinGecko. It applies machine learning techniquesâ€”especially **XGBoost regression**â€”to forecast cryptocurrency behavior and identify potential market instability due to liquidity risks.

---

## ğŸš€ Project Objective

The goal is to build a robust ML pipeline that:
- Ingests and preprocesses real-world cryptocurrency data
- Performs Exploratory Data Analysis (EDA)
- Engineers informative features (e.g., rolling stats, time features)
- Selects, tunes, and evaluates multiple regression models
- Deploys the best model locally using **Streamlit**
- Provides insights into model performance and prediction reliability

---

## ğŸ—‚ï¸ Project Structure

cryptocurrency_price_prediction/
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # Raw CSVs from CoinGecko
â”‚ â””â”€â”€ processed/ # Cleaned and feature-processed files
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ 01_data_preprocessing.ipynb
â”‚ â”œâ”€â”€ 02_eda.ipynb
â”‚ â”œâ”€â”€ 03_feature_engineering.ipynb
â”‚ â”œâ”€â”€ 04_model_selection.ipynb
â”‚ â”œâ”€â”€ 05_model_training.ipynb
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ data_loader.py
â”‚ â”œâ”€â”€ data_processor.py
â”‚ â”œâ”€â”€ feature_engineer.py
â”‚ â”œâ”€â”€ models.py
â”‚ â””â”€â”€ evaluator.py
â”‚ 
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ final_xgboost_model.pkl
â”‚ â”œâ”€â”€ feature_columns.pkl
â”œâ”€â”€ deployment/
â”‚ â”œâ”€â”€ app.py # Streamlit app for testing
â”‚ â””â”€â”€ requirements.txt
â”œâ”€â”€ reports/
â”‚ â”œâ”€â”€ eda_report.pdf
â”‚ â”œâ”€â”€ hld_document.pdf
â”‚ â”œâ”€â”€ lld_document.pdf
â”‚ â”œâ”€â”€ pipeline_architecture.pdf
â”‚ â””â”€â”€ final_report.pdf
â””â”€â”€ README.md


---

## ğŸ§  Machine Learning Workflow

### âœ… Feature Engineering
- Time-based features: year, month, day\_of\_week, quarter
- Volatility: % change over 1h, 24h, 7d
- Aggregated stats: rolling mean, min, max (future-proof for extension)

### âœ… Model Selection
- Compared Linear Regression, Ridge, Lasso, Random Forest, LightGBM, and XGBoost
- Used RMSE, MAE, and RÂ² as evaluation metrics
- **XGBoost** selected as final model

### âœ… Hyperparameter Tuning
- Used `GridSearchCV` and `RandomizedSearchCV`
- Outcome: tuning did not always improve metricsâ€”highlighting the importance of baseline performance checks

---

## ğŸ“ˆ Results

| Metric | Before Tuning | After Tuning |
|--------|---------------|--------------|
| MAE    | 1730.02       | **1211.72**  |
| RMSE   | 6870.40       | **5203.52**  |
| RÂ²     | -0.8996       | **-0.0897**  |

*Insight: Hyperparameter tuning helped but gains were modest due to data limitations.*

---

## ğŸ’» Local Deployment

Launch the Streamlit app:
```bash
streamlit run deployment/app.py
